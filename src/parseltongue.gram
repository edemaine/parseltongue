# PEG grammar for Parseltongue,
# based on the PEG grammar for Python in pegen/python.gram

@header '''#!/usr/bin/env python3.9

import ast
from typing import List
from dataclasses import dataclass
from pegen.parser import *
import lexer

class waiting_for_py310: pass
ast.match_case = waiting_for_py310
ast.pattern = waiting_for_py310

@dataclass
class NameDefaultPair:
  arg: ast.arg
  value: ast.expr

@dataclass
class KeyValuePair:
  key: ast.expr
  value: ast.expr

@dataclass
class KeyPatternPair:
  key: ast.expr
  pattern: ast.pattern

@dataclass
class CmpopExprPair:
  cmpop: ast.cmpop
  expr: ast.expr

@dataclass
class KeywordOrStarred:
  keyword: Any
  starred: int

@dataclass
class SlashWithDefault:
  plain_names: List[ast.arg]
  names_with_default: list

@dataclass
class StarEtc:
  vararg: ast.arg
  kwonlyargs: List[NameDefaultPair]
  kwarg: ast.arg
TypeComment = str

def seq_count_dots(seq) -> int:
  dots = 0
  for token in seq:
    if token.type == 'ELLIPSIS':
      dots += 3
    elif token.type == 'DOT':
      dots += 1
  return dots

def seq_flatten(seq):
  return [item for list in seq for item in list]

def set_expr_context(expr, context):
  expr.context = context()
  return expr

def empty_arguments():
  return ast.arguments([], [], None, [], [], None, posdefaults)

def map_names_to_ids(names):
  return [name.id for name in names]

def class_def_decorators(decorators: List[ast.expr], class_def: ast.stmt) -> ast.stmt:
  return ast.ClassDef(class_def.name, class_def.bases, class_def.keywords,
    class_def.body, decorators, **copy_locations(class_def))

def add_type_comment_to_arg(arg, tc):
  if not tc: return arg
  tc = TypeComment(tc)
  return ast.arg(arg.arg, arg.annotation, tc, **copy_locations(arg))

def make_arguments(slash_without_default: List[ast.arg],
    slash_with_default: SlashWithDefault,
    plain_names: List[ast.arg],
    names_with_default: List,
    star_etc: StarEtc):
  # xxx
  return ast.arguments()

def check_legacy_stmt(x):
  return x.id in ['print', 'exec']

def copy_locations(x):
  return dict(
    lineno = x.lineno,
    col_offset = x.col_offset,
    end_lineno = x.end_lineno,
    end_col_offset = x.end_col_offset,
  )

# Wrap tokens in ast structures
class Parser(Parser):
  def name(self) -> Optional[ast.Name]:
    name = super().name()
    if name: name = ast.Name(name)
    return name

'''

@trailer '''
def main():
  for filename in sys.argv[1:]:
    file = open(filename, 'r')
    parser = GeneratedParser(lexer.Tokenizer(file, filename), verbose=True)
    parsed = parser.file()
    print(ast.dump(parsed))

if __name__ == '__main__': main()
'''

# ========================= START OF THE GRAMMAR =========================

# General grammatical elements and rules:
#
# * Strings with double quotes (") denote SOFT KEYWORDS
# * Strings with single quotes (') denote KEYWORDS
# * Upper case names (NAME) denote tokens in the Grammar/Tokens file
# * Rule names starting with "invalid_" are used for specialized syntax errors
#     - These rules are NOT used in the first pass of the parser.
#     - Only if the first pass fails to parse, a second pass including the invalid
#       rules will be executed.
#     - If the parser fails in the second phase with a generic syntax error, the
#       location of the generic failure of the first pass will be used (this avoids
#       reporting incorrect locations due to the invalid rules).
#     - The order of the alternatives involving invalid rules matter
#       (like any rule in PEG).
#
# Grammar Syntax (see PEP 617 for more information):
#
# rule_name: expression
#   Optionally, a type can be included right after the rule name, which
#   specifies the return type of the C or Python function corresponding to the
#   rule:
# rule_name[return_type]: expression
#   If the return type is omitted, then a void * is returned in C and an Any in
#   Python.
# e1 e2
#   Match e1, then match e2.
# e1 | e2
#   Match e1 or e2.
#   The first alternative can also appear on the line after the rule name for
#   formatting purposes. In that case, a | must be used before the first
#   alternative, like so:
#       rule_name[return_type]:
#            | first_alt
#            | second_alt
# ( e )
#   Match e (allows also to use other operators in the group like '(e)*')
# [ e ] or e?
#   Optionally match e.
# e*
#   Match zero or more occurrences of e.
# e+
#   Match one or more occurrences of e.
# s.e+
#   Match one or more occurrences of e, separated by s. The generated parse tree
#   does not include the separator. This is otherwise identical to (e (s e)*).
# &e
#   Succeed if e can be parsed, without consuming any input.
# !e
#   Fail if e can be parsed, without consuming any input.
# ~
#   Commit to the current alternative, even if it fails to parse.
#

# STARTING RULES
# ==============

file[ast.mod]: a=[statements] ENDMARKER { ast.Module(a) }
interactive[ast.mod]: a=statement_newline { ast.Interactive(a, p.arena) }
eval[ast.mod]: a=expressions NEWLINE* ENDMARKER { ast.Expression(a, p.arena) }
func_type[ast.mod]: '(' a=[type_expressions] ')' '->' b=expression NEWLINE* ENDMARKER { ast.FunctionType(a, b, p.arena) }
fstring[ast.expr]: star_expressions

# GENERAL STATEMENTS
# ==================

statements[List[ast.stmt]]: a=statement+ { seq_flatten(a) }

statement[List[ast.stmt]]: a=compound_stmt { [a] }
    | a[List[ast.stmt]]=simple_stmts { a }

statement_newline[List[ast.stmt]]:
    | a=compound_stmt NEWLINE { [a] }
    | simple_stmts
    | NEWLINE { [ast.Pass(LOCATIONS)] }
    | ENDMARKER { _PyPegen_interactive_exit(p) }

simple_stmts[List[ast.stmt]]:
    | a=simple_stmt !';' NEWLINE { [a] } # Not needed, there for speedup
    | a[List[ast.stmt]]=';'.simple_stmt+ [';'] NEWLINE { a }

# NOTE: assignment MUST precede expression, else parsing a simple assignment
# will throw a SyntaxError.
simple_stmt[ast.stmt] (memo):
    | assignment
    | e=star_expressions { ast.Expr(e, LOCATIONS) }
    | &'return' return_stmt
    | &('import' | 'from') import_stmt
    | &'raise' raise_stmt
    | 'pass' { ast.Pass(LOCATIONS) }
    | &'del' del_stmt
    | &'yield' yield_stmt
    | &'assert' assert_stmt
    | 'break' { ast.Break(LOCATIONS) }
    | 'continue' { ast.Continue(LOCATIONS) }
    | &'global' global_stmt
    | &'nonlocal' nonlocal_stmt

compound_stmt[ast.stmt]:
    | &('def' | '@' | ASYNC) function_def
    | &'if' if_stmt
    | &('class' | '@') class_def
    | &('with' | ASYNC) with_stmt
    | &('for' | ASYNC) for_stmt
    | &'try' try_stmt
    | &'while' while_stmt
    | match_stmt

# SIMPLE STATEMENTS
# =================

# NOTE: annotated_rhs may start with 'yield'; yield_expr must start with 'yield'
assignment[ast.stmt]:
    | a=NAME ':' b=expression c=['=' d=annotated_rhs { d }] {
        ast.AnnAssign(set_expr_context(a, ast.Store), b, c, 1, LOCATIONS)
      }
    | a=('(' b=single_target ')' { b }
         | single_subscript_attribute_target) ':' b=expression c=['=' d=annotated_rhs { d }] {
        ast.AnnAssign(a, b, c, 0, LOCATIONS)
      }
    | a[List[ast.expr]]=(z=star_targets '=' { z })+ b=(yield_expr | star_expressions) !'=' tc=[TYPE_COMMENT] {
        ast.Assign(a, b, TypeComment(tc), LOCATIONS)
      }
    | a=single_target b=augassign ~ c=(yield_expr | star_expressions) {
        ast.AugAssign(a, b.kind, c, LOCATIONS) }
    | invalid_assignment

annotated_rhs[ast.expr]: yield_expr | star_expressions

augassign[ast.operator]:
    | '+=' { ast.Add(LOCATIONS) }
    | '-=' { ast.Sub(LOCATIONS) }
    | '*=' { ast.Mult(LOCATIONS) }
    | '@=' { ast.MatMult(LOCATIONS) }
    | '/=' { ast.Div(LOCATIONS) }
    | '%=' { ast.Mod(LOCATIONS) }
    | '&=' { ast.BitAnd(LOCATIONS) }
    | '|=' { ast.BitOr(LOCATIONS) }
    | '^=' { ast.BitXor(LOCATIONS) }
    | '<<=' { ast.LShift(LOCATIONS) }
    | '>>=' { ast.RShift(LOCATIONS) }
    | '**=' { ast.Pow(LOCATIONS) }
    | '//=' { ast.FloorDiv(LOCATIONS) }

return_stmt[ast.stmt]:
    | 'return' a=[star_expressions] { ast.Return(a, LOCATIONS) }

raise_stmt[ast.stmt]:
    | 'raise' a=expression b=['from' z=expression { z }] { ast.Raise(a, b, LOCATIONS) }
    | 'raise' { ast.Raise(None, None, LOCATIONS) }

global_stmt[ast.stmt]: 'global' a[List[ast.expr]]=','.NAME+ {
    ast.Global(map_names_to_ids(a), LOCATIONS) }

nonlocal_stmt[ast.stmt]: 'nonlocal' a[List[ast.expr]]=','.NAME+ {
    ast.Nonlocal(map_names_to_ids(a), LOCATIONS) }

del_stmt[ast.stmt]:
    | 'del' a=del_targets &(';' | NEWLINE) { ast.Delete(a, LOCATIONS) }
    | invalid_del_stmt

yield_stmt[ast.stmt]: y=yield_expr { ast.Expr(y, LOCATIONS) }

assert_stmt[ast.stmt]: 'assert' a=expression b=[',' z=expression { z }] { ast.Assert(a, b, LOCATIONS) }

import_stmt[ast.stmt]: import_name | import_from

# Import statements
# -----------------

import_name[ast.stmt]: 'import' a=dotted_as_names { ast.Import(a, LOCATIONS) }
# note below: the ('.' | '...') is necessary because '...' is tokenized as ELLIPSIS
import_from[ast.stmt]:
    | 'from' a=('.' | '...')* b=dotted_name 'import' c=import_from_targets {
        ast.ImportFrom(b.id, c, seq_count_dots(a), LOCATIONS) }
    | 'from' a=('.' | '...')+ 'import' b=import_from_targets {
        ast.ImportFrom(None, b, seq_count_dots(a), LOCATIONS) }
import_from_targets[List[ast.alias]]:
    | '(' a=import_from_as_names [','] ')' { a }
    | import_from_as_names !','
    | '*' { [ast.alias('*', LOCATIONS)] }
    | invalid_import_from_targets
import_from_as_names[List[ast.alias]]:
    | a[List[ast.alias]]=','.import_from_as_name+ { a }
import_from_as_name[ast.alias]:
    | a=NAME b=['as' z=NAME { z }] { ast.alias(a.id, b.id if b else None, LOCATIONS) }
dotted_as_names[List[ast.alias]]:
    | a[List[ast.alias]]=','.dotted_as_name+ { a }
dotted_as_name[ast.alias]:
    | a=dotted_name b=['as' z=NAME { z }] { ast.alias(a.id, b.id if b else None, LOCATIONS) }
dotted_name[ast.expr]:
    | a=dotted_name '.' b=NAME { ast.Name(f'{a.id}.{b.id}') }
    | NAME

# COMPOUND STATEMENTS
# ===================

# Common elements
# ---------------

block[List[ast.stmt]] (memo):
    | NEWLINE INDENT a=statements DEDENT { a }
    | simple_stmts
    | invalid_block

decorators[List[ast.expr]]: a[List[ast.expr]]=('@' f=named_expression NEWLINE { f })+ { a }

# Class definitions
# -----------------

class_def[ast.stmt]:
    | a=decorators b=class_def_raw { class_def_decorators(a, b) }
    | class_def_raw

class_def_raw[ast.stmt]:
    | invalid_class_def_raw
    | 'class' a=NAME b=['(' z=[arguments] ')' { z }] &&':' c=block {
        ast.ClassDef(a.id,
          b.args if b else None,
          b.keywords if b else None,
          c, None, LOCATIONS) }

# Function definitions
# --------------------

function_def[ast.stmt]:
    | d=decorators f=function_def_raw { _PyPegen_function_def_decorators(p, d, f) }
    | function_def_raw

function_def_raw[ast.stmt]:
    | invalid_def_raw
    | 'def' n=NAME '(' params=[params] ')' a=['->' z=expression { z }] &&':' tc=[func_type_comment] b=block {
        ast.FunctionDef(n.id,
                        params if params else empty_arguments(),
                        b, None, a, TypeComment(tc), LOCATIONS) }
    | ASYNC 'def' n=NAME '(' params=[params] ')' a=['->' z=expression { z }] &&':' tc=[func_type_comment] b=block {
        CHECK_VERSION(
            ast.stmt,
            5,
            "Async functions are",
            ast.AsyncFunctionDef(n.id,
                            params if params else empty_arguments(),
                            b, None, a, TypeComment(tc), LOCATIONS)
        ) }

# Function parameters
# -------------------

params[ast.arguments]:
    | invalid_parameters
    | parameters

parameters[ast.arguments]:
    | a=slash_no_default b[List[ast.arg]]=param_no_default* c=param_with_default* d=[star_etc] {
        make_arguments(a, None, b, c, d) }
    | a=slash_with_default b=param_with_default* c=[star_etc] {
        make_arguments(None, a, None, b, c) }
    | a[List[ast.arg]]=param_no_default+ b=param_with_default* c=[star_etc] {
        make_arguments(None, None, a, b, c) }
    | a=param_with_default+ b=[star_etc] {
        make_arguments(None, None, None, a, b) }
    | a=star_etc { make_arguments(None, None, None, None, a) }

# Some duplication here because we can't write (',' | &')'),
# which is because we don't support empty alternatives (yet).

slash_no_default[List[ast.arg]]:
    | a[List[ast.arg]]=param_no_default+ '/' ',' { a }
    | a[List[ast.arg]]=param_no_default+ '/' &')' { a }
slash_with_default[SlashWithDefault]:
    | a=param_no_default* b=param_with_default+ '/' ',' { SlashWithDefault(a, b) }
    | a=param_no_default* b=param_with_default+ '/' &')' { SlashWithDefault(a, b) }

star_etc[StarEtc]:
    | '*' a=param_no_default b=param_maybe_default* c=[kwds] {
        StarEtc(a, b, c) }
    | '*' ',' b=param_maybe_default+ c=[kwds] {
        StarEtc(None, b, c) }
    | a=kwds { StarEtc(None, None, a) }
    | invalid_star_etc

kwds[ast.arg]: '**' a=param_no_default { a }

# One parameter.  This *includes* a following comma and type comment.
#
# There are three styles:
# - No default
# - With default
# - Maybe with default
#
# There are two alternative forms of each, to deal with type comments:
# - Ends in a comma followed by an optional type comment
# - No comma, optional type comment, must be followed by close paren
# The latter form is for a final parameter without trailing comma.
#

param_no_default[ast.arg]:
    | a=param ',' tc=TYPE_COMMENT? { add_type_comment_to_arg(a, tc) }
    | a=param tc=TYPE_COMMENT? &')' { add_type_comment_to_arg(a, tc) }
param_with_default[NameDefaultPair]:
    | a=param c=default ',' tc=TYPE_COMMENT? { NameDefaultPair(a, c, tc) }
    | a=param c=default tc=TYPE_COMMENT? &')' { NameDefaultPair(a, c, tc) }
param_maybe_default[NameDefaultPair]:
    | a=param c=default? ',' tc=TYPE_COMMENT? { NameDefaultPair(a, c, tc) }
    | a=param c=default? tc=TYPE_COMMENT? &')' { NameDefaultPair(a, c, tc) }
param[ast.arg]: a=NAME b=annotation? { ast.arg(a.id, b, None, LOCATIONS) }
annotation[ast.expr]: ':' a=expression { a }
default[ast.expr]: '=' a=expression { a }

# If statement
# ------------

if_stmt[ast.stmt]:
    | invalid_if_stmt
    | 'if' a=named_expression ':' b=block c=elif_stmt {
        ast.If(a, b, CHECK(List[ast.stmt], [c]), LOCATIONS) }
    | 'if' a=named_expression ':' b=block c=[else_block] { ast.If(a, b, c, LOCATIONS) }
elif_stmt[ast.stmt]:
    | invalid_elif_stmt
    | 'elif' a=named_expression ':' b=block c=elif_stmt {
        ast.If(a, b, CHECK(List[ast.stmt], [c]), LOCATIONS) }
    | 'elif' a=named_expression ':' b=block c=[else_block] { ast.If(a, b, c, LOCATIONS) }
else_block[List[ast.stmt]]:
    | invalid_else_stmt
    | 'else' &&':' b=block { b }

# While statement
# ---------------

while_stmt[ast.stmt]:
    | invalid_while_stmt
    | 'while' a=named_expression ':' b=block c=[else_block] { ast.While(a, b, c, LOCATIONS) }

# For statement
# -------------

for_stmt[ast.stmt]:
    | invalid_for_stmt
    | 'for' t=star_targets 'in' ~ ex=star_expressions &&':' tc=[TYPE_COMMENT] b=block el=[else_block] {
        ast.For(t, ex, b, el, TypeComment(tc), LOCATIONS) }
    | ASYNC 'for' t=star_targets 'in' ~ ex=star_expressions &&':' tc=[TYPE_COMMENT] b=block el=[else_block] {
        CHECK_VERSION(ast.stmt, 5, "Async for loops are", ast.AsyncFor(t, ex, b, el, TypeComment(tc), LOCATIONS)) }
    | invalid_for_target

# With statement
# --------------

with_stmt[ast.stmt]:
    | invalid_with_stmt_indent
    | 'with' '(' a[List[ast.withitem]]=','.with_item+ ','? ')' ':' b=block {
        ast.With(a, b, None, LOCATIONS) }
    | 'with' a[List[ast.withitem]]=','.with_item+ ':' tc=[TYPE_COMMENT] b=block {
        ast.With(a, b, TypeComment(tc), LOCATIONS) }
    | ASYNC 'with' '(' a[List[ast.withitem]]=','.with_item+ ','? ')' ':' b=block {
       CHECK_VERSION(ast.stmt, 5, "Async with statements are", ast.AsyncWith(a, b, None, LOCATIONS)) }
    | ASYNC 'with' a[List[ast.withitem]]=','.with_item+ ':' tc=[TYPE_COMMENT] b=block {
       CHECK_VERSION(ast.stmt, 5, "Async with statements are", ast.AsyncWith(a, b, TypeComment(tc), LOCATIONS)) }
    | invalid_with_stmt

with_item[ast.withitem]:
    | e=expression 'as' t=star_target &(',' | ')' | ':') { _PyAST_withitem(e, t, p.arena) }
    | invalid_with_item
    | e=expression { _PyAST_withitem(e, None, p.arena) }

# Try statement
# -------------

try_stmt[ast.stmt]:
    | invalid_try_stmt
    | 'try' &&':' b=block f=finally_block { ast.Try(b, None, None, f, LOCATIONS) }
    | 'try' &&':' b=block ex[List[ast.excepthandler]]=except_block+ el=[else_block] f=[finally_block] { ast.Try(b, ex, el, f, LOCATIONS) }

# Except statement
# ----------------

except_block[ast.excepthandler]:
    | invalid_except_stmt_indent
    | 'except' e=expression t=['as' z=NAME { z }] ':' b=block {
        ast.ExceptHandler(e, t.id if t else None, b, LOCATIONS) }
    | 'except' ':' b=block { ast.ExceptHandler(None, None, b, LOCATIONS) }
    | invalid_except_stmt
finally_block[List[ast.stmt]]:
    | invalid_finally_stmt
    | 'finally' &&':' a=block { a }

# Match statement
# ---------------

match_stmt[ast.stmt]:
    | "match" subject=subject_expr ':' NEWLINE INDENT cases[List[ast.match_case]]=case_block+ DEDENT {
        CHECK_VERSION(ast.stmt, 10, "Pattern matching is", ast.Match(subject, cases, LOCATIONS)) }
    | invalid_match_stmt

subject_expr[ast.expr]:
    | value=star_named_expression ',' values=star_named_expressions? {
        ast.Tuple([value] + values, ast.Load, LOCATIONS) }
    | named_expression

case_block[ast.match_case]:
    | invalid_case_block
    | "case" pattern=patterns guard=guard? ':' body=block {
        ast.match_case(pattern, guard, body, p.arena) }

guard[ast.expr]: 'if' guard=named_expression { guard }

patterns[ast.pattern]:
    | patterns[List[ast.pattern]]=open_sequence_pattern {
        ast.MatchSequence(patterns, LOCATIONS) }
    | pattern

pattern[ast.pattern]:
    | as_pattern
    | or_pattern

as_pattern[ast.pattern]:
    | pattern=or_pattern 'as' target=pattern_capture_target {
        ast.MatchAs(pattern, target.id, LOCATIONS) }
    | invalid_as_pattern

or_pattern[ast.pattern]:
    | patterns[List[ast.pattern]]='|'.closed_pattern+ {
        patterns[0] if len(patterns) == 1 else ast.MatchOr(patterns, LOCATIONS) }

closed_pattern[ast.pattern]:
    | literal_pattern
    | capture_pattern
    | wildcard_pattern
    | value_pattern
    | group_pattern
    | sequence_pattern
    | mapping_pattern
    | class_pattern

# Literal patterns are used for equality and identity constraints
literal_pattern[ast.pattern]:
    | value=signed_number !('+' | '-') { ast.MatchValue(value, LOCATIONS) }
    | value=complex_number { ast.MatchValue(value, LOCATIONS) }
    | value=strings { ast.MatchValue(value, LOCATIONS) }
    | 'None' { ast.MatchSingleton(Py_None, LOCATIONS) }
    | 'True' { ast.MatchSingleton(Py_True, LOCATIONS) }
    | 'False' { ast.MatchSingleton(Py_False, LOCATIONS) }

# Literal expressions are used to restrict permitted mapping pattern keys
literal_expr[ast.expr]:
    | signed_number !('+' | '-')
    | complex_number
    | strings
    | 'None' { ast.Constant(Py_None, None, LOCATIONS) }
    | 'True' { ast.Constant(Py_True, None, LOCATIONS) }
    | 'False' { ast.Constant(Py_False, None, LOCATIONS) }

complex_number[ast.expr]:
    | real=signed_real_number '+' imag=imaginary_number {
        ast.BinOp(real, Add, imag, LOCATIONS) }
    | real=signed_real_number '-' imag=imaginary_number  {
        ast.BinOp(real, Sub, imag, LOCATIONS) }

signed_number[ast.expr]:
    | NUMBER
    | '-' number=NUMBER { ast.UnaryOp(USub, number, LOCATIONS) }

signed_real_number[ast.expr]:
    | real_number
    | '-' real=real_number { ast.UnaryOp(USub, real, LOCATIONS) }

real_number[ast.expr]:
    | real=NUMBER { _PyPegen_ensure_real(p, real) }

imaginary_number[ast.expr]:
    | imag=NUMBER { _PyPegen_ensure_imaginary(p, imag) }

capture_pattern[ast.pattern]:
    | target=pattern_capture_target { ast.MatchAs(None, target.id, LOCATIONS) }

pattern_capture_target[ast.expr]:
    | !"_" name=NAME !('.' | '(' | '=') {
        set_expr_context(name, ast.Store) }

wildcard_pattern[ast.pattern]:
    | "_" { ast.MatchAs(None, None, LOCATIONS) }

value_pattern[ast.pattern]:
    | attr=attr !('.' | '(' | '=') { ast.MatchValue(attr, LOCATIONS) }

attr[ast.expr]:
    | value=name_or_attr '.' attr=NAME {
        ast.Attribute(value, attr.id, ast.Load, LOCATIONS) }

name_or_attr[ast.expr]:
    | attr
    | NAME

group_pattern[ast.pattern]:
    | '(' pattern=pattern ')' { pattern }

sequence_pattern[ast.pattern]:
    | '[' patterns=maybe_sequence_pattern? ']' { ast.MatchSequence(patterns, LOCATIONS) }
    | '(' patterns=open_sequence_pattern? ')' { ast.MatchSequence(patterns, LOCATIONS) }

open_sequence_pattern[List[ast.AST]]:
    | pattern=maybe_star_pattern ',' patterns=maybe_sequence_pattern? {
        [pattern] + patterns }

maybe_sequence_pattern[List[ast.AST]]:
    | patterns=','.maybe_star_pattern+ ','? { patterns }

maybe_star_pattern[ast.pattern]:
    | star_pattern
    | pattern

star_pattern[ast.pattern]:
    | '*' target=pattern_capture_target {
        ast.MatchStar(target.id, LOCATIONS) }
    | '*' wildcard_pattern {
        ast.MatchStar(None, LOCATIONS) }

mapping_pattern[ast.pattern]:
    | '{' '}' {
        ast.MatchMapping(None, None, None, LOCATIONS) }
    | '{' rest=double_star_pattern ','? '}' {
        ast.MatchMapping(None, None, rest.id, LOCATIONS) }
    | '{' items=items_pattern ',' rest=double_star_pattern ','? '}' {
        ast.MatchMapping(
            CHECK(List[ast.expr], _PyPegen_get_pattern_keys(p, items)),
            CHECK(List[ast.pattern], _PyPegen_get_patterns(p, items)),
            rest.id,
            LOCATIONS) }
    | '{' items=items_pattern ','? '}' {
        ast.MatchMapping(
            CHECK(List[ast.expr], _PyPegen_get_pattern_keys(p, items)),
            CHECK(List[ast.pattern], _PyPegen_get_patterns(p, items)),
            None,
            LOCATIONS) }

items_pattern[List[ast.AST]]:
    | ','.key_value_pattern+

key_value_pattern[List[KeyPatternPair]]:
    | key=(literal_expr | attr) ':' pattern=pattern {
        _PyPegen_key_pattern_pair(p, key, pattern) }

double_star_pattern[ast.expr]:
    | '**' target=pattern_capture_target { target }

class_pattern[ast.pattern]:
    | cls=name_or_attr '(' ')' {
        ast.MatchClass(cls, None, None, None, LOCATIONS) }
    | cls=name_or_attr '(' patterns=positional_patterns ','? ')' {
        ast.MatchClass(cls, patterns, None, None, LOCATIONS) }
    | cls=name_or_attr '(' keywords=keyword_patterns ','? ')' {
        ast.MatchClass(
            cls, None,
            map_names_to_ids(_PyPegen_get_pattern_keys(p, keywords)),
            CHECK(List[ast.pattern], _PyPegen_get_patterns(p, keywords)),
            LOCATIONS) }
    | cls=name_or_attr '(' patterns=positional_patterns ',' keywords=keyword_patterns ','? ')' {
        ast.MatchClass(
            cls,
            patterns,
            map_names_to_ids(_PyPegen_get_pattern_keys(p, keywords)),
            CHECK(List[ast.pattern], _PyPegen_get_patterns(p, keywords)),
            LOCATIONS) }
    | invalid_class_pattern

positional_patterns[List[ast.pattern]]:
    | args[List[ast.pattern]]=','.pattern+ { args }

keyword_patterns[List[ast.AST]]:
    | ','.keyword_pattern+

keyword_pattern[List[KeyPatternPair]]:
    | arg=NAME '=' value=pattern { _PyPegen_key_pattern_pair(p, arg, value) }

# EXPRESSIONS
# -----------

expressions[ast.expr]:
    | a=expression b=(',' c=expression { c })+ [','] {
        ast.Tuple([a] + b, ast.Load, LOCATIONS) }
    | a=expression ',' { ast.Tuple([a], ast.Load, LOCATIONS) }
    | expression

expression[ast.expr] (memo):
    | invalid_expression
    | a=disjunction 'if' b=disjunction 'else' c=expression { ast.IfExp(b, a, c, LOCATIONS) }
    | disjunction
    | lambdef

yield_expr[ast.expr]:
    | 'yield' 'from' a=expression { ast.YieldFrom(a, LOCATIONS) }
    | 'yield' a=[star_expressions] { ast.Yield(a, LOCATIONS) }

star_expressions[ast.expr]:
    | a=star_expression b=(',' c=star_expression { c })+ [','] {
        ast.Tuple([a] + b, ast.Load, LOCATIONS) }
    | a=star_expression ',' { ast.Tuple([a], ast.Load, LOCATIONS) }
    | star_expression

star_expression[ast.expr] (memo):
    | '*' a=bitwise_or { ast.Starred(a, ast.Load, LOCATIONS) }
    | expression

star_named_expressions[List[ast.expr]]: a[List[ast.expr]]=','.star_named_expression+ [','] { a }

star_named_expression[ast.expr]:
    | '*' a=bitwise_or { ast.Starred(a, ast.Load, LOCATIONS) }
    | named_expression

assigment_expression[ast.expr]:
    | a=NAME ':=' ~ b=expression { ast.NamedExpr(CHECK(ast.expr, set_expr_context(a, ast.Store)), b, LOCATIONS) }

named_expression[ast.expr]:
    | assigment_expression
    | invalid_named_expression
    | expression !':='

disjunction[ast.expr] (memo):
    | a=conjunction b=('or' c=conjunction { c })+ { ast.BoolOp(
        Or, [a] + b, LOCATIONS) }
    | conjunction

conjunction[ast.expr] (memo):
    | a=inversion b=('and' c=inversion { c })+ { ast.BoolOp(
        And, [a] + b, LOCATIONS) }
    | inversion

inversion[ast.expr] (memo):
    | 'not' a=inversion { ast.UnaryOp(Not, a, LOCATIONS) }
    | comparison

# Comparisons operators
# ---------------------

comparison[ast.expr]:
    | a=bitwise_or b=compare_op_bitwise_or_pair+ {
        ast.Compare(
            a,
            CHECK(List[int], _PyPegen_get_cmpops(p, b)),
            CHECK(List[ast.expr], _PyPegen_get_exprs(p, b)),
            LOCATIONS) }
    | bitwise_or

compare_op_bitwise_or_pair[CmpopExprPair]:
    | eq_bitwise_or
    | noteq_bitwise_or
    | lte_bitwise_or
    | lt_bitwise_or
    | gte_bitwise_or
    | gt_bitwise_or
    | notin_bitwise_or
    | in_bitwise_or
    | isnot_bitwise_or
    | is_bitwise_or

eq_bitwise_or[CmpopExprPair]: '==' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, Eq, a) }
noteq_bitwise_or[CmpopExprPair]:
    | (tok='!=' { None if _PyPegen_check_barry_as_flufl(p, tok) else tok}) a=bitwise_or {_PyPegen_cmpop_expr_pair(p, NotEq, a) }
lte_bitwise_or[CmpopExprPair]: '<=' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, LtE, a) }
lt_bitwise_or[CmpopExprPair]: '<' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, Lt, a) }
gte_bitwise_or[CmpopExprPair]: '>=' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, GtE, a) }
gt_bitwise_or[CmpopExprPair]: '>' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, Gt, a) }
notin_bitwise_or[CmpopExprPair]: 'not' 'in' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, NotIn, a) }
in_bitwise_or[CmpopExprPair]: 'in' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, In, a) }
isnot_bitwise_or[CmpopExprPair]: 'is' 'not' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, IsNot, a) }
is_bitwise_or[CmpopExprPair]: 'is' a=bitwise_or { _PyPegen_cmpop_expr_pair(p, Is, a) }

# Logical operators
# -----------------

bitwise_or[ast.expr]:
    | a=bitwise_or '|' b=bitwise_xor { ast.BinOp(a, BitOr, b, LOCATIONS) }
    | bitwise_xor

bitwise_xor[ast.expr]:
    | a=bitwise_xor '^' b=bitwise_and { ast.BinOp(a, BitXor, b, LOCATIONS) }
    | bitwise_and

bitwise_and[ast.expr]:
    | a=bitwise_and '&' b=shift_expr { ast.BinOp(a, BitAnd, b, LOCATIONS) }
    | shift_expr

shift_expr[ast.expr]:
    | a=shift_expr '<<' b=sum { ast.BinOp(a, LShift, b, LOCATIONS) }
    | a=shift_expr '>>' b=sum { ast.BinOp(a, RShift, b, LOCATIONS) }
    | sum

# Arithmetic operators
# --------------------

sum[ast.expr]:
    | a=sum '+' b=term { ast.BinOp(a, Add, b, LOCATIONS) }
    | a=sum '-' b=term { ast.BinOp(a, Sub, b, LOCATIONS) }
    | term

term[ast.expr]:
    | a=term '*' b=factor { ast.BinOp(a, Mult, b, LOCATIONS) }
    | a=term '/' b=factor { ast.BinOp(a, Div, b, LOCATIONS) }
    | a=term '//' b=factor { ast.BinOp(a, FloorDiv, b, LOCATIONS) }
    | a=term '%' b=factor { ast.BinOp(a, Mod, b, LOCATIONS) }
    | a=term '@' b=factor { CHECK_VERSION(ast.expr, 5, "The '@' operator is", ast.BinOp(a, MatMult, b, LOCATIONS)) }
    | factor

factor[ast.expr] (memo):
    | '+' a=factor { ast.UnaryOp(UAdd, a, LOCATIONS) }
    | '-' a=factor { ast.UnaryOp(USub, a, LOCATIONS) }
    | '~' a=factor { ast.UnaryOp(Invert, a, LOCATIONS) }
    | power

power[ast.expr]:
    | a=await_primary '**' b=factor { ast.BinOp(a, Pow, b, LOCATIONS) }
    | await_primary

# Primary elements
# ----------------

# Primary elements are things like "obj.something.something", "obj[something]", "obj(something)", "obj" ...

await_primary[ast.expr] (memo):
    | AWAIT a=primary { CHECK_VERSION(ast.expr, 5, "Await expressions are", ast.Await(a, LOCATIONS)) }
    | primary

primary[ast.expr]:
    | a=primary '.' b=NAME { ast.Attribute(a, b.id, ast.Load, LOCATIONS) }
    | a=primary b=genexp { ast.Call(a, [b], None, LOCATIONS) }
    | a=primary '(' b=[arguments] ')' {
        ast.Call(a,
                 b.args if b else None,
                 b.keywords if b else None,
                 LOCATIONS) }
    | a=primary '[' b=slices ']' { ast.Subscript(a, b, ast.Load, LOCATIONS) }
    | atom

slices[ast.expr]:
    | a=slice !',' { a }
    | a[List[ast.expr]]=','.slice+ [','] { ast.Tuple(a, ast.Load, LOCATIONS) }

slice[ast.expr]:
    | a=[expression] ':' b=[expression] c=[':' d=[expression] { d }] { ast.Slice(a, b, c, LOCATIONS) }
    | a=named_expression { a }

atom[ast.expr]:
    | NAME
    | 'True' { ast.Constant(Py_True, None, LOCATIONS) }
    | 'False' { ast.Constant(Py_False, None, LOCATIONS) }
    | 'None' { ast.Constant(Py_None, None, LOCATIONS) }
    | &STRING strings
    | NUMBER
    | &'(' (tuple | group | genexp)
    | &'[' (list | listcomp)
    | &'{' (dict | set | dictcomp | setcomp)
    | '...' { ast.Constant(Py_Ellipsis, None, LOCATIONS) }

group[ast.expr]:
    | '(' a=(yield_expr | named_expression) ')' { a }
    | invalid_group

# Lambda functions
# ----------------

lambdef[ast.expr]:
    | 'lambda' a=[lambda_params] ':' b=expression {
        ast.Lambda(a if a else empty_arguments(), b, LOCATIONS) }

lambda_params[ast.arguments]:
    | invalid_lambda_parameters
    | lambda_parameters

# lambda_parameters etc. duplicates parameters but without annotations
# or type comments, and if there's no comma after a parameter, we expect
# a colon, not a close parenthesis.  (For more, see parameters above.)
#
lambda_parameters[ast.arguments]:
    | a=lambda_slash_no_default b[List[ast.arg]]=lambda_param_no_default* c=lambda_param_with_default* d=[lambda_star_etc] {
        make_arguments(a, None, b, c, d) }
    | a=lambda_slash_with_default b=lambda_param_with_default* c=[lambda_star_etc] {
        make_arguments(None, a, None, b, c) }
    | a[List[ast.arg]]=lambda_param_no_default+ b=lambda_param_with_default* c=[lambda_star_etc] {
        make_arguments(None, None, a, b, c) }
    | a=lambda_param_with_default+ b=[lambda_star_etc] { make_arguments(None, None, None, a, b)}
    | a=lambda_star_etc { make_arguments(None, None, None, None, a) }

lambda_slash_no_default[List[ast.arg]]:
    | a[List[ast.arg]]=lambda_param_no_default+ '/' ',' { a }
    | a[List[ast.arg]]=lambda_param_no_default+ '/' &':' { a }

lambda_slash_with_default[SlashWithDefault]:
    | a=lambda_param_no_default* b=lambda_param_with_default+ '/' ',' { SlashWithDefault(a, b) }
    | a=lambda_param_no_default* b=lambda_param_with_default+ '/' &':' { SlashWithDefault(a, b) }

lambda_star_etc[StarEtc]:
    | '*' a=lambda_param_no_default b=lambda_param_maybe_default* c=[lambda_kwds] {
        StarEtc(a, b, c) }
    | '*' ',' b=lambda_param_maybe_default+ c=[lambda_kwds] {
        StarEtc(None, b, c) }
    | a=lambda_kwds { StarEtc(None, None, a) }
    | invalid_lambda_star_etc

lambda_kwds[ast.arg]: '**' a=lambda_param_no_default { a }

lambda_param_no_default[ast.arg]:
    | a=lambda_param ',' { a }
    | a=lambda_param &':' { a }
lambda_param_with_default[NameDefaultPair]:
    | a=lambda_param c=default ',' { NameDefaultPair(a, c, None) }
    | a=lambda_param c=default &':' { NameDefaultPair(a, c, None) }
lambda_param_maybe_default[NameDefaultPair]:
    | a=lambda_param c=default? ',' { NameDefaultPair(a, c, None) }
    | a=lambda_param c=default? &':' { NameDefaultPair(a, c, None) }
lambda_param[ast.arg]: a=NAME { ast.arg(a.id, None, None, LOCATIONS) }

# LITERALS
# ========

strings[ast.expr] (memo): a=STRING+ { a
    # xxx _PyPegen_concatenate_strings(p, a)
    }

list[ast.expr]:
    | '[' a=[star_named_expressions] ']' { ast.List(a, ast.Load, LOCATIONS) }

tuple[ast.expr]:
    | '(' a=[y=star_named_expression ',' z=[star_named_expressions] { [y] + z } ] ')' {
        ast.Tuple(a, ast.Load, LOCATIONS) }

set[ast.expr]: '{' a=star_named_expressions '}' { ast.Set(a, LOCATIONS) }

# Dicts
# -----

dict[ast.expr]:
    | '{' a=[double_starred_kvpairs] '}' {
        ast.Dict(
            CHECK(List[ast.expr], _PyPegen_get_keys(p, a)),
            CHECK(List[ast.expr], _PyPegen_get_values(p, a)),
            LOCATIONS) }
    | '{' invalid_double_starred_kvpairs '}'

double_starred_kvpairs[List]: a=','.double_starred_kvpair+ [','] { a }

double_starred_kvpair[KeyValuePair]:
    | '**' a=bitwise_or { KeyValuePair(None, a) }
    | kvpair

kvpair[KeyValuePair]: a=expression ':' b=expression { KeyValuePair(a, b) }

# Comprehensions & Generators
# ---------------------------

for_if_clauses[List[ast.comprehension]]:
    | a[List[ast.comprehension]]=for_if_clause+ { a }

for_if_clause[ast.comprehension]:
    | ASYNC 'for' a=star_targets 'in' ~ b=disjunction c[List[ast.expr]]=('if' z=disjunction { z })* {
        CHECK_VERSION(ast.comprehension, 6, "Async comprehensions are", _PyAST_comprehension(a, b, c, 1, p.arena)) }
    | 'for' a=star_targets 'in' ~ b=disjunction c[List[ast.expr]]=('if' z=disjunction { z })* {
        _PyAST_comprehension(a, b, c, 0, p.arena) }
    | invalid_for_target

listcomp[ast.expr]:
    | '[' a=named_expression b=for_if_clauses ']' { ast.ListComp(a, b, LOCATIONS) }
    | invalid_comprehension

setcomp[ast.expr]:
    | '{' a=named_expression b=for_if_clauses '}' { ast.SetComp(a, b, LOCATIONS) }
    | invalid_comprehension

genexp[ast.expr]:
    | '(' a=( assigment_expression | expression !':=') b=for_if_clauses ')' { ast.GeneratorExp(a, b, LOCATIONS) }
    | invalid_comprehension

dictcomp[ast.expr]:
    | '{' a=kvpair b=for_if_clauses '}' { ast.DictComp(a.key, a.value, b, LOCATIONS) }
    | invalid_dict_comprehension

# FUNCTION CALL ARGUMENTS
# =======================

arguments[ast.expr] (memo):
    | a=args [','] &')' { a }
    | invalid_arguments

args[ast.expr]:
    | a[List[ast.expr]]=','.(starred_expression | ( assigment_expression | expression !':=') !'=')+ b=[',' k=kwargs {k}] {
        ast.Call(None, a, b, LOCATIONS) #xxx
        #_PyPegen_collect_call_seqs(p, a, b, LOCATIONS)
      }
    | a=kwargs { ast.Call(_PyPegen_dummy_name(p),
                          CHECK_NULL_ALLOWED(List[ast.expr], _PyPegen_seq_extract_starred_exprs(p, a)),
                          CHECK_NULL_ALLOWED(List[ast.keyword], _PyPegen_seq_delete_starred_exprs(p, a)),
                          LOCATIONS) }

kwargs[List[ast.AST]]:
    | a=','.kwarg_or_starred+ ',' b=','.kwarg_or_double_starred+ { _PyPegen_join_sequences(p, a, b) }
    | ','.kwarg_or_starred+
    | ','.kwarg_or_double_starred+

starred_expression[ast.expr]:
    | '*' a=expression { ast.Starred(a, ast.Load, LOCATIONS) }

kwarg_or_starred[KeywordOrStarred]:
    | invalid_kwarg
    | a=NAME '=' b=expression {
        KeywordOrStarred(ast.keyword(a.id, b, LOCATIONS), 1) }
    | a=starred_expression { KeywordOrStarred(a, 0) }

kwarg_or_double_starred[KeywordOrStarred]:
    | invalid_kwarg
    | a=NAME '=' b=expression {
        KeywordOrStarred(ast.keyword(a.id, b, LOCATIONS), 1) }
    | '**' a=expression { KeywordOrStarred(ast.keyword(None, a, LOCATIONS), 1) }

# ASSIGNMENT TARGETS
# ==================

# Generic targets
# ---------------

# NOTE: star_targets may contain *bitwise_or, targets may not.
star_targets[ast.expr]:
    | a=star_target !',' { a }
    | a=star_target b=(',' c=star_target { c })* [','] {
        ast.Tuple([a] + b, ast.Store, LOCATIONS) }

star_targets_list_seq[List[ast.expr]]: a[List[ast.expr]]=','.star_target+ [','] { a }

star_targets_tuple_seq[List[ast.expr]]:
    | a=star_target b=(',' c=star_target { c })+ [','] { [a] + b }
    | a=star_target ',' { [a] }

star_target[ast.expr] (memo):
    | '*' a=(!'*' star_target) {
        ast.Starred(set_expr_context(a, ast.Store), ast.Store, LOCATIONS) }
    | target_with_star_atom

target_with_star_atom[ast.expr] (memo):
    | a=t_primary '.' b=NAME !t_lookahead { ast.Attribute(a, b.id, ast.Store, LOCATIONS) }
    | a=t_primary '[' b=slices ']' !t_lookahead { ast.Subscript(a, b, ast.Store, LOCATIONS) }
    | star_atom

star_atom[ast.expr]:
    | a=NAME { set_expr_context(a, ast.Store) }
    | '(' a=target_with_star_atom ')' { set_expr_context(a, ast.Store) }
    | '(' a=[star_targets_tuple_seq] ')' { ast.Tuple(a, ast.Store, LOCATIONS) }
    | '[' a=[star_targets_list_seq] ']' { ast.List(a, ast.Store, LOCATIONS) }

single_target[ast.expr]:
    | single_subscript_attribute_target
    | a=NAME { set_expr_context(a, ast.Store) }
    | '(' a=single_target ')' { a }

single_subscript_attribute_target[ast.expr]:
    | a=t_primary '.' b=NAME !t_lookahead { ast.Attribute(a, b.id, ast.Store, LOCATIONS) }
    | a=t_primary '[' b=slices ']' !t_lookahead { ast.Subscript(a, b, ast.Store, LOCATIONS) }

t_primary[ast.expr]:
    | a=t_primary '.' b=NAME &t_lookahead { ast.Attribute(a, b.id, ast.Load, LOCATIONS) }
    | a=t_primary '[' b=slices ']' &t_lookahead { ast.Subscript(a, b, ast.Load, LOCATIONS) }
    | a=t_primary b=genexp &t_lookahead {
        ast.Call(a, [b], None, LOCATIONS) }
    | a=t_primary '(' b=[arguments] ')' &t_lookahead {
        ast.Call(a,
                 b.args if b else None,
                 b.keywords if b else None,
                 LOCATIONS) }
    | a=atom &t_lookahead { a }

t_lookahead: '(' | '[' | '.'

# Targets for del statements
# --------------------------

del_targets[List[ast.expr]]: a[List[ast.expr]]=','.del_target+ [','] { a }

del_target[ast.expr] (memo):
    | a=t_primary '.' b=NAME !t_lookahead { ast.Attribute(a, b.id, Del, LOCATIONS) }
    | a=t_primary '[' b=slices ']' !t_lookahead { ast.Subscript(a, b, Del, LOCATIONS) }
    | del_t_atom

del_t_atom[ast.expr]:
    | a=NAME { set_expr_context(a, Del) }
    | '(' a=del_target ')' { set_expr_context(a, Del) }
    | '(' a=[del_targets] ')' { ast.Tuple(a, Del, LOCATIONS) }
    | '[' a=[del_targets] ']' { ast.List(a, Del, LOCATIONS) }

# TYPING ELEMENTS
# ---------------

# type_expressions allow */** but ignore them
type_expressions[List[ast.expr]]:
    | a=','.expression+ ',' '*' b=expression ',' '**' c=expression {
        a + [b, c] }
    | a=','.expression+ ',' '*' b=expression { a + [b] }
    | a=','.expression+ ',' '**' b=expression { a + [b] }
    | '*' a=expression ',' '**' b=expression { a + [b] }
    | '*' a=expression { [a] }
    | '**' a=expression { [a] }
    | a[List[ast.expr]]=','.expression+ {a}

func_type_comment[lexer.Token]:
    | NEWLINE t=TYPE_COMMENT &(NEWLINE INDENT) { t }  # Must be followed by indented block
    | invalid_double_type_comments
    | TYPE_COMMENT

# ========================= END OF THE GRAMMAR ===========================



# ========================= START OF INVALID RULES =======================

# From here on, there are rules for invalid syntax with specialised error messages
invalid_arguments:
    | a=args ',' '*' { RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "iterable argument unpacking follows keyword argument unpacking") }
    | a=expression b=for_if_clauses ',' [args | expression for_if_clauses] {
        RAISE_SYNTAX_ERROR_KNOWN_RANGE(a, PyPegen_last_item(b, ast.comprehension).target, "Generator expression must be parenthesized") }
    | a=NAME b='=' expression for_if_clauses {
        RAISE_SYNTAX_ERROR_KNOWN_RANGE(a, b, "invalid syntax. Maybe you meant '==' or ':=' instead of '='?")}
    | a=args for_if_clauses { _PyPegen_nonparen_genexp_in_call(p, a) }
    | args ',' a=expression b=for_if_clauses {
        RAISE_SYNTAX_ERROR_KNOWN_RANGE(a, b[len(b) - 1].target, "Generator expression must be parenthesized") }
    | a=args ',' args { _PyPegen_arguments_parsing_error(p, a) }
invalid_kwarg:
    | a=NAME b='=' expression for_if_clauses {
        RAISE_SYNTAX_ERROR_KNOWN_RANGE(a, b, "invalid syntax. Maybe you meant '==' or ':=' instead of '='?")}
    | !(NAME '=') a=expression b='=' {
        RAISE_SYNTAX_ERROR_KNOWN_RANGE(
            a, b, "expression cannot contain assignment, perhaps you meant \"==\"?") }

expression_without_invalid[ast.expr]:
    | a=disjunction 'if' b=disjunction 'else' c=expression { ast.IfExp(b, a, c, LOCATIONS) }
    | disjunction
    | lambdef
invalid_legacy_expression:
    | a=NAME !'(' b=star_expressions {
        RAISE_SYNTAX_ERROR_KNOWN_RANGE(a, b,
            "Missing parentheses in call to '%U'. Did you mean %U(...)?",
            a.id, a.id) if check_legacy_stmt(a) else None }

invalid_expression:
   | invalid_legacy_expression
    # !(NAME STRING) is not matched so we don't show this error with some invalid string prefixes like: kf"dsfsdf"
    # Soft keywords need to also be ignored because they can be parsed as NAME NAME
   | !(NAME STRING | SOFT_KEYWORD) a=disjunction b=expression_without_invalid {
        RAISE_SYNTAX_ERROR_KNOWN_RANGE(a, b, "invalid syntax. Perhaps you forgot a comma?") }
   | a=disjunction 'if' b=disjunction !('else'|':') { RAISE_SYNTAX_ERROR_KNOWN_RANGE(a, b, "expected 'else' after 'if' expression") }

invalid_named_expression:
    | a=expression ':=' expression {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(
            a, "cannot use assignment expressions with %s", _PyPegen_get_expr_name(a)) }
    | a=NAME '=' b=bitwise_or !('='|':=') {
        None if p.in_raw_rule else RAISE_SYNTAX_ERROR_KNOWN_RANGE(a, b, "invalid syntax. Maybe you meant '==' or ':=' instead of '='?") }
    | !(list|tuple|genexp|'True'|'None'|'False') a=bitwise_or b='=' bitwise_or !('='|':=') {
        None if p.in_raw_rule else RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "cannot assign to %s here. Maybe you meant '==' instead of '='?",
                                          _PyPegen_get_expr_name(a)) }

invalid_assignment:
    | a=invalid_ann_assign_target ':' expression {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(
            a,
            "only single target (not %s) can be annotated",
            _PyPegen_get_expr_name(a)
        )}
    | a=star_named_expression ',' star_named_expressions* ':' expression {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "only single target (not tuple) can be annotated") }
    | a=expression ':' expression {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "illegal target for annotation") }
    | (star_targets '=')* a=star_expressions '=' {
        RAISE_SYNTAX_ERROR_INVALID_TARGET(STAR_TARGETS, a) }
    | (star_targets '=')* a=yield_expr '=' { RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "assignment to yield expression not possible") }
    | a=star_expressions augassign (yield_expr | star_expressions) {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(
            a,
            "'%s' is an illegal expression for augmented assignment",
            _PyPegen_get_expr_name(a)
        )}
invalid_ann_assign_target[ast.expr]:
    | list
    | tuple
    | '(' a=invalid_ann_assign_target ')' { a }
invalid_del_stmt:
    | 'del' a=star_expressions {
        RAISE_SYNTAX_ERROR_INVALID_TARGET(DEL_TARGETS, a) }
invalid_block:
    | NEWLINE !INDENT { RAISE_INDENTATION_ERROR("expected an indented block") }
invalid_comprehension:
    | ('[' | '(' | '{') a=starred_expression for_if_clauses {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "iterable unpacking cannot be used in comprehension") }
    | ('[' | '{') a=star_named_expression ',' b=star_named_expressions for_if_clauses {
        RAISE_SYNTAX_ERROR_KNOWN_RANGE(a, PyPegen_last_item(b, ast.expr),
        "did you forget parentheses around the comprehension target?") }
    | ('[' | '{') a=star_named_expression b=',' for_if_clauses {
        RAISE_SYNTAX_ERROR_KNOWN_RANGE(a, b, "did you forget parentheses around the comprehension target?") }
invalid_dict_comprehension:
    | '{' a='**' bitwise_or for_if_clauses '}' {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "dict unpacking cannot be used in dict comprehension") }
invalid_parameters:
    | param_no_default* invalid_parameters_helper a=param_no_default {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "non-default argument follows default argument") }
invalid_parameters_helper: # This is only there to avoid type errors
    | a=slash_with_default { [a] }
    | param_with_default+
invalid_lambda_parameters:
    | lambda_param_no_default* invalid_lambda_parameters_helper a=lambda_param_no_default {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "non-default argument follows default argument") }
invalid_lambda_parameters_helper:
    | a=lambda_slash_with_default { [a] }
    | lambda_param_with_default+
invalid_star_etc:
    | a='*' (')' | ',' (')' | '**')) { RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "named arguments must follow bare *") }
    | '*' ',' TYPE_COMMENT { RAISE_SYNTAX_ERROR("bare * has associated type comment") }
invalid_lambda_star_etc:
    | '*' (':' | ',' (':' | '**')) { RAISE_SYNTAX_ERROR("named arguments must follow bare *") }
invalid_double_type_comments:
    | TYPE_COMMENT NEWLINE TYPE_COMMENT NEWLINE INDENT {
        RAISE_SYNTAX_ERROR("Cannot have two type comments on def") }
invalid_with_item:
    | expression 'as' a=expression &(',' | ')' | ':') {
        RAISE_SYNTAX_ERROR_INVALID_TARGET(STAR_TARGETS, a) }

invalid_for_target:
    | ASYNC? 'for' a=star_expressions {
        RAISE_SYNTAX_ERROR_INVALID_TARGET(FOR_TARGETS, a) }

invalid_group:
    | '(' a=starred_expression ')' {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "cannot use starred expression here") }
    | '(' a='**' expression ')' {
        RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "cannot use double starred expression here") }
invalid_import_from_targets:
    | import_from_as_names ',' NEWLINE {
        RAISE_SYNTAX_ERROR("trailing comma not allowed without surrounding parentheses") }

invalid_with_stmt:
    | [ASYNC] 'with' ','.(expression ['as' star_target])+ &&':'
    | [ASYNC] 'with' '(' ','.(expressions ['as' star_target])+ ','? ')' &&':'
invalid_with_stmt_indent:
    | [ASYNC] a='with' ','.(expression ['as' star_target])+ ':' NEWLINE !INDENT {
        RAISE_INDENTATION_ERROR("expected an indented block after 'with' statement on line %d", a.lineno) }
    | [ASYNC] a='with' '(' ','.(expressions ['as' star_target])+ ','? ')' ':' NEWLINE !INDENT {
        RAISE_INDENTATION_ERROR("expected an indented block after 'with' statement on line %d", a.lineno) }

invalid_try_stmt:
    | a='try' ':' NEWLINE !INDENT {
        RAISE_INDENTATION_ERROR("expected an indented block after 'try' statement on line %d", a.lineno) }
    | 'try' ':' block !('except' | 'finally') { RAISE_SYNTAX_ERROR("expected 'except' or 'finally' block") }
invalid_except_stmt:
    | 'except' a=expression ',' expressions ['as' NAME ] ':' {
        RAISE_SYNTAX_ERROR_STARTING_FROM(a, "multiple exception types must be parenthesized") }
    | a='except' expression ['as' NAME ] NEWLINE { RAISE_SYNTAX_ERROR("expected ':'") }
    | a='except' NEWLINE { RAISE_SYNTAX_ERROR("expected ':'") }
invalid_finally_stmt:
    | a='finally' ':' NEWLINE !INDENT {
        RAISE_INDENTATION_ERROR("expected an indented block after 'finally' statement on line %d", a.lineno) }
invalid_except_stmt_indent:
    | a='except' expression ['as' NAME ] ':' NEWLINE !INDENT {
        RAISE_INDENTATION_ERROR("expected an indented block after 'except' statement on line %d", a.lineno) }
    | a='except' ':' NEWLINE !INDENT { RAISE_SYNTAX_ERROR("expected an indented block after except statement on line %d", a.lineno) }
invalid_match_stmt:
    | "match" subject_expr !':' { CHECK_VERSION(None, 10, "Pattern matching is", RAISE_SYNTAX_ERROR("expected ':'") ) }
    | a="match" subject=subject_expr ':' NEWLINE !INDENT {
        RAISE_INDENTATION_ERROR("expected an indented block after 'match' statement on line %d", a.lineno) }
invalid_case_block:
    | "case" patterns guard? !':' { RAISE_SYNTAX_ERROR("expected ':'") }
    | a="case" patterns guard? ':' NEWLINE !INDENT {
        RAISE_INDENTATION_ERROR("expected an indented block after 'case' statement on line %d", a.lineno) }
invalid_as_pattern:
    | or_pattern 'as' a="_" { RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "cannot use '_' as a target") }
    | or_pattern 'as' !NAME a=expression { RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "invalid pattern target") }
invalid_class_pattern:
    | name_or_attr '(' a=invalid_class_argument_pattern  { RAISE_SYNTAX_ERROR_KNOWN_RANGE(
        PyPegen_first_item(a, ast.pattern),
        PyPegen_last_item(a, ast.pattern),
        "positional patterns follow keyword patterns") }
invalid_class_argument_pattern[List[ast.pattern]]:
    | [positional_patterns ','] keyword_patterns ',' a=positional_patterns { a }
invalid_if_stmt:
    | 'if' named_expression NEWLINE { RAISE_SYNTAX_ERROR("expected ':'") }
    | a='if' a=named_expression ':' NEWLINE !INDENT {
        RAISE_INDENTATION_ERROR("expected an indented block after 'if' statement on line %d", a.lineno) }
invalid_elif_stmt:
    | 'elif' named_expression NEWLINE { RAISE_SYNTAX_ERROR("expected ':'") }
    | a='elif' named_expression ':' NEWLINE !INDENT {
        RAISE_INDENTATION_ERROR("expected an indented block after 'elif' statement on line %d", a.lineno) }
invalid_else_stmt:
    | a='else' ':' NEWLINE !INDENT {
        RAISE_INDENTATION_ERROR("expected an indented block after 'else' statement on line %d", a.lineno) }
invalid_while_stmt:
    | 'while' named_expression NEWLINE { RAISE_SYNTAX_ERROR("expected ':'") }
    | a='while' named_expression ':' NEWLINE !INDENT {
        RAISE_INDENTATION_ERROR("expected an indented block after 'while' statement on line %d", a.lineno) }
invalid_for_stmt:
    | [ASYNC] a='for' star_targets 'in' star_expressions ':' NEWLINE !INDENT {
        RAISE_INDENTATION_ERROR("expected an indented block after 'for' statement on line %d", a.lineno) }
invalid_def_raw:
    | [ASYNC] a='def' NAME '(' [params] ')' ['->' expression] ':' NEWLINE !INDENT {
        RAISE_INDENTATION_ERROR("expected an indented block after function definition on line %d", a.lineno) }
invalid_class_def_raw:
    | a='class' NAME ['('[arguments] ')'] ':' NEWLINE !INDENT {
        RAISE_INDENTATION_ERROR("expected an indented block after class definition on line %d", a.lineno) }

invalid_double_starred_kvpairs:
    | ','.double_starred_kvpair+ ',' invalid_kvpair
    | expression ':' a='*' bitwise_or { RAISE_SYNTAX_ERROR_STARTING_FROM(a, "cannot use a starred expression in a dictionary value") }
    | expression a=':' &('}'|',') { RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "expression expected after dictionary key and ':'") }
invalid_kvpair:
    | a=expression !(':') {
        RAISE_ERROR_KNOWN_LOCATION(p, PyExc_SyntaxError, a.lineno, a.end_col_offset - 1, a.end_lineno, -1, "':' expected after dictionary key") }
    | expression ':' a='*' bitwise_or { RAISE_SYNTAX_ERROR_STARTING_FROM(a, "cannot use a starred expression in a dictionary value") }
    | expression a=':' {RAISE_SYNTAX_ERROR_KNOWN_LOCATION(a, "expression expected after dictionary key and ':'") }
